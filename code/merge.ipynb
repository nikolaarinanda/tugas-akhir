{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8499bbae-f3ea-47bf-a639-2aafa433b57e",
   "metadata": {},
   "source": [
    "## 1. Download Packages & Resource\n",
    "Download packages yang diperlukan untuk preprocessing, model ataupun training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2d53e72f-898f-4897-80a9-7b9e46095370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR SOME SECTION\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# FOR PREPROCESSING SECTION\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from simplemma import text_lemmatize\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# FOR MODEL SECTION\n",
    "from torch import nn # from torch import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# FOR TRAIN SECTION\n",
    "import wandb\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "# from muon import SingleDeviceMuonWithAuxAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "dd0d807d-d231-4931-bac9-9851ae84fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK yang diperlukan\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "STOPWORDS = set(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d65bd8-81d4-4744-b4cd-7861880f1afe",
   "metadata": {},
   "source": [
    "## 2. Dataset Class\n",
    "Kelas dari dataset yang mana dilakukan proses preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "82320a68-87c0-4bd4-91ea-fff1a08098ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyberbullyingDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            file_path=\"../dataset/cyberbullying.csv\",\n",
    "            tokenizer_name=\"indobenchmark/indobert-base-p1\",\n",
    "            folds_file=\"k_folds.json\",\n",
    "            random_state=29082002,\n",
    "            split=\"train\",\n",
    "            fold=0,\n",
    "            n_folds=5,\n",
    "            max_length=128,\n",
    "            augmentasi_file=\"../dataset/dictionary/augmentation.json\",\n",
    "            slang_word_file=\"../dataset/dictionary/slang-word-specific.json\",\n",
    "    ):        \n",
    "        self.file_path = file_path\n",
    "        self.folds_file = folds_file\n",
    "        self.random_state = random_state\n",
    "        self.split = split\n",
    "        self.fold = fold\n",
    "        self.n_folds = n_folds\n",
    "        self.max_length = max_length\n",
    "        self.augmentasi_data = self.load_file(augmentasi_file)\n",
    "        self.slang_dict = self.load_file(slang_word_file)\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "\n",
    "        # Load dataset\n",
    "        self.load_data()\n",
    "        # Setup n-Fold Cross Validation\n",
    "        self.setup_folds()\n",
    "        # Mempersiapkan Indices (bentuk jamak index)\n",
    "        self.setup_indices()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Mengambil index dari data yang akan diambil\n",
    "        idx = self.indices[idx]\n",
    "        # Mengambil data komentar dan sentiment\n",
    "        text = str(self.df.iloc[idx][\"comment\"])\n",
    "        label = str(self.df.iloc[idx][\"sentiment\"])\n",
    "        # Melakukan Pre-Processing\n",
    "        comment_processed = self.preprocess(text)\n",
    "        # Tokenisasi\n",
    "        encoding = self.tokenizer(\n",
    "            comment_processed,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        data = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(int(label), dtype=torch.long),\n",
    "            'original_text': text,\n",
    "            'processed_text': comment_processed,\n",
    "            'original_index': idx\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def load_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "        \n",
    "        # Load dictionary slang dari file JSON\n",
    "        with open('slang_dictionary.json', 'r') as file:\n",
    "            slang_dict = json.load(file)\n",
    "\n",
    "    def aeda_augment(self, text):\n",
    "        \"\"\"\n",
    "        Melakukan augmentasi teks dengan metode AEDA:\n",
    "        Menyisipkan tanda baca secara acak di posisi acak dalam teks.\n",
    "        \"\"\"\n",
    "        punctuations = [\".\", \";\", \"?\", \":\", \"!\", \",\"]\n",
    "        words = text.split()\n",
    "        if len(words) == 0:\n",
    "            return text\n",
    "    \n",
    "        # # Tentukan berapa banyak tanda baca yang akan disisipkan\n",
    "        # n_insert = random.randint(1, max(1, len(words) // 3))\n",
    "        \n",
    "        # # Pilih posisi acak untuk sisipan\n",
    "        # positions = random.sample(range(len(words)), n_insert)\n",
    "        # positions.sort(reverse=True)  # disisipkan dari belakang biar indeks tidak bergeser\n",
    "    \n",
    "        # for pos in positions:\n",
    "        #     punct = random.choice(punctuations)\n",
    "        #     words.insert(pos, punct)\n",
    "\n",
    "        # Pilih posisi acak untuk sisipan\n",
    "        position = random.randint(0, len(words) - 1)\n",
    "        \n",
    "        # Pilih tanda baca acak\n",
    "        punct = random.choice(punctuations)\n",
    "        \n",
    "        # Sisipkan tanda baca ke dalam list kata\n",
    "        words.insert(position, punct)\n",
    "    \n",
    "        return \" \".join(words)\n",
    "\n",
    "    def random_typo(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) < 1:\n",
    "            return text\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        word = words[idx]\n",
    "        if len(word) > 1:\n",
    "            char_list = list(word)\n",
    "            i = random.randint(0, len(char_list) - 2)\n",
    "            char_list[i], char_list[i+1] = char_list[i+1], char_list[i]  # swap 2 huruf berdekatan\n",
    "            words[idx] = ''.join(char_list)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def random_swap(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            return text\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def random_delete(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) <= 1:\n",
    "            return text\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        del words[idx]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def augmentation_text(self, text, probability=0.5):\n",
    "        # Hanya lakukan augmentasi dengan probabilitas tertentu\n",
    "        if random.random() > probability:\n",
    "            return text\n",
    "\n",
    "        # PROBABILITAS ACAK\n",
    "        # Daftar semua fungsi augmentasi yang tersedia\n",
    "        augmentations = [\n",
    "            self.aeda_augment,\n",
    "            self.random_typo,\n",
    "            self.random_swap,\n",
    "            self.random_delete\n",
    "        ]\n",
    "        \n",
    "        # Pecah kalimat menjadi kumpulan kata\n",
    "        words = text.split()\n",
    "        # Tentukan berapa banyak augmentasi yang akan dilakukan\n",
    "        n_insert = random.randint(1, max(1, len(words) // 3))\n",
    "        for i in range(n_insert):\n",
    "            # Pilih satu augmentasi secara acak\n",
    "            augmentation_func = random.choice(augmentations)\n",
    "            # Terapkan augmentasi yang dipilih\n",
    "            text = augmentation_func(text)\n",
    "        return text\n",
    "\n",
    "        # # Pilih satu augmentasi secara acak\n",
    "        # augmentation_func = random.choice(augmentations)\n",
    "        # # Mengembalikan augmentasi yang dipilih\n",
    "        # return augmentation_func(text)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Konversi ke huruf kecil\n",
    "        text = text.lower()\n",
    "\n",
    "        # Hapus mention (@...) dan hashtag (#...) => ada kolom comment yang #VALUE!\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "        # # Hapus emoji dan karakter non-ASCII\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "        # Stemming\n",
    "        # factory = StemmerFactory()\n",
    "        # stemmer = factory.create_stemmer()\n",
    "        # text   = stemmer.stem(text)\n",
    "\n",
    "        # Augmentasi\n",
    "        text = self.augmentation_text(text)\n",
    "        \n",
    "        # Tokenisasi\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Lemmatization\n",
    "        # lemmatizer = WordNetLemmatizer()\n",
    "        # words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "        # Menghapus stopwords\n",
    "        words = [word for word in words if word not in STOPWORDS]\n",
    "\n",
    "        # Menggabungkan kembali kata-kata menjadi kalimat\n",
    "        text = ' '.join(words)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def setup_indices(self):\n",
    "        '''\n",
    "        Mempersiapkan indices untuk data yang akan di-training\n",
    "        '''\n",
    "        fold_key = f\"fold_{self.fold}\"\n",
    "        if self.split == \"train\":\n",
    "            self.indices = self.fold_indices[fold_key]['train_indices']\n",
    "        else:\n",
    "            self.indices = self.fold_indices[fold_key]['val_indices']\n",
    "\n",
    "    def setup_folds(self):\n",
    "        # Jika fold sudah ada, maka load fold\n",
    "        if os.path.exists(self.folds_file):\n",
    "            self.load_folds()\n",
    "        # Jika tidak ada, maka buat fold\n",
    "        else:\n",
    "            self.create_folds()\n",
    "\n",
    "    def load_folds(self):\n",
    "        '''\n",
    "        Apabila fold sudah ada, maka load fold\n",
    "        '''\n",
    "        with open(self.folds_file, 'r') as f:\n",
    "            fold_data = json.load(f)\n",
    "        self.fold_indices = fold_data['fold_indices']\n",
    "        print(f\"Menggunakan {fold_data['n_folds']} folds dengan {fold_data['n_samples']} samples\")\n",
    "    \n",
    "    def create_folds(self):\n",
    "        '''\n",
    "        Apabila fold sudah ada, maka load fold\n",
    "        '''\n",
    "        print(f\"Membuat n-fold CV dengan random state {self.random_state}\")\n",
    "        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "        # print(\"\\nStratified k-fold positif samples per fold:\")\n",
    "        # for _, val_idx in skf.split(self.df, self.df['sentiment']):\n",
    "        #     print(f\"{np.sum(self.df['sentiment'].iloc[val_idx] == 1)} out of {len(val_idx)}\")\n",
    "\n",
    "        fold_indices = {}\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(self.df, self.df['sentiment'])):\n",
    "            fold_indices[f\"fold_{fold}\"] = {\n",
    "                'train_indices': train_idx.tolist(),\n",
    "                'val_indices': val_idx.tolist()\n",
    "            }\n",
    "        \n",
    "        # Simpan fold ke file\n",
    "        with open(self.folds_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'fold_indices': fold_indices,\n",
    "                'n_samples': len(self.df),\n",
    "                'n_folds': self.n_folds,\n",
    "                'random_state': self.random_state\n",
    "            }, f)\n",
    "\n",
    "            self.fold_indices = fold_indices\n",
    "            print(f'Created {self.n_folds}-fold indices and saved to {self.folds_file}')\n",
    "\n",
    "    def load_data(self):\n",
    "        print(f'Loading data from {self.file_path}...')\n",
    "        self.df = pd.read_csv(self.file_path) # Load csv\n",
    "        # self.df.columns = ['sentiment', 'comment'] # Rename columns\n",
    "        if len(self.df.columns) == 2:\n",
    "            self.df.columns = ['sentiment', 'comment']\n",
    "        else:\n",
    "            print(\"âš ï¸ Jumlah kolom tidak sesuai, kolom asli:\", self.df.columns)\n",
    "        self.df = self.df.dropna(subset=['sentiment', 'comment']) # Drop NaN values\n",
    "        self.df['sentiment'] = self.df['sentiment'].astype(int) # Convert sentiment to int\n",
    "        self.df['sentiment'] = self.df['sentiment'].apply(lambda x: 1 if x == -1 else 0) # Transform labels: convert -1 to 1, and 1 to 0\n",
    "        self.df = self.df[(self.df['sentiment'] == 0) | (self.df['sentiment'] == 1)] # Filter sentiment\n",
    "\n",
    "        # Undersampling menyeimbangkan dataset (hanya untuk split \"train\")\n",
    "        if self.split == \"train\":\n",
    "            df_label_0 = self.df[self.df['sentiment'] == 0]\n",
    "            df_label_1 = self.df[self.df['sentiment'] == 1]\n",
    "\n",
    "            min_samples_per_class = min(len(df_label_0), len(df_label_1))\n",
    "\n",
    "            df_label_0_undersampled = df_label_0.sample(n=min_samples_per_class, random_state=self.random_state)\n",
    "            df_label_1_undersampled = df_label_1.sample(n=min_samples_per_class, random_state=self.random_state)\n",
    "\n",
    "            self.df = pd.concat([df_label_0_undersampled, df_label_1_undersampled])\n",
    "            self.df = self.df.sample(frac=1, random_state=self.random_state).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f8440-e9fb-45db-8db8-abdf16a2d8c0",
   "metadata": {},
   "source": [
    "## 2x. Main Section Preprocessing\n",
    "Bagian utama untuk menjalankan program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a4d60e57-bdbd-438a-af17-5f3416a6b252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     dataset = CyberbullyingDataset(fold=0, split=\"train\")\n",
    "#     data = dataset[0]\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bb4fe-c97f-4469-bb36-c55ccbdce845",
   "metadata": {},
   "source": [
    "## 3. Model Class\n",
    "Kelas dari model machine learning yang akan di training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b006a7f5-3871-44e1-a3e2-1d8b7e8a80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNNLight(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNNLight is a lightweight version of TextCNN for text classification.\n",
    "    vocab_size: Size of the vocabulary\n",
    "    embed_dim: Dimension of the word embeddings\n",
    "    num_classes: Number of output classes\n",
    "    output_dim: Dimension of the output after convolution\n",
    "    kernel_size: List of kernel sizes for convolutional layers\n",
    "    This model uses two convolutional layers with kernel sizes 3 and 4,panjang jendela (window) digunakan filter untuk melihat urutan kata secara lokal.\n",
    "    followed by max pooling and a fully connected layer.\n",
    "    The output dimension is set to 64, and dropout is applied to prevent overfitting.\n",
    "    The model is designed to be efficient and suitable for smaller datasets or real-time applications.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_classes=2, output_dim=64, dropout_rate=0.3):\n",
    "        super(TextCNNLight, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.conv1 = nn.Conv1d(embed_dim, output_dim, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(embed_dim, output_dim, kernel_size=4)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear((output_dim * 2), num_classes)\n",
    "\n",
    "    def forward(self, x): # Input dan output layer tidak ditulis eksplisit\n",
    "        x = self.embedding(x) # Embedding\n",
    "        x = x.permute(0, 2, 1) # Permute\n",
    "        x1 = F.relu(self.conv1(x)).max(dim=2)[0] # Convolution 1\n",
    "        x2 = F.relu(self.conv2(x)).max(dim=2)[0] # Convolution 2\n",
    "        x = torch.cat((x1, x2), dim=1) # Concatenate (gabungkan fitur dari 2 konvolusi)\n",
    "        x = self.dropout(x) # Dropout\n",
    "        return self.fc(x) # Fully Connected\n",
    "\n",
    "# PERCOBAAN 1\n",
    "class TextCNNLightResNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNNLightResNorm\n",
    "    - Gabungan TextCNNLight + BatchNorm + Residual connection\n",
    "    - Stabil, ringan, cocok untuk teks pendek seperti komentar media sosial.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_classes=2, output_dim=64, dropout_rate=0.3):\n",
    "        super(TextCNNLightResNorm, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        # Dua layer konvolusi dengan ukuran kernel berbeda\n",
    "        self.conv1 = nn.Conv1d(embed_dim, output_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(embed_dim, output_dim, kernel_size=4, padding=1)\n",
    "\n",
    "        # Normalisasi batch setelah konvolusi\n",
    "        self.bn1 = nn.BatchNorm1d(output_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "        # Shortcut projection agar dimensi sama (residual)\n",
    "        self.shortcut = nn.Linear(embed_dim, output_dim * 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(output_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1ï¸âƒ£ Embedding\n",
    "        x_embed = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        x_embed_t = x_embed.permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
    "\n",
    "        # 2ï¸âƒ£ Convolution + ReLU + BatchNorm + Global Max Pooling\n",
    "        x1 = F.relu(self.bn1(self.conv1(x_embed_t))).max(dim=2)[0]\n",
    "        x2 = F.relu(self.bn2(self.conv2(x_embed_t))).max(dim=2)[0]\n",
    "\n",
    "        # 3ï¸âƒ£ Concatenate hasil konvolusi\n",
    "        x_cat = torch.cat((x1, x2), dim=1)  # (batch, output_dim * 2)\n",
    "\n",
    "        # 4ï¸âƒ£ Residual connection: proyeksikan embedding ke dimensi yang sama\n",
    "        residual = self.shortcut(x_embed.mean(dim=1))  # rata-rata embedding â†’ dim (output_dim*2)\n",
    "        x_res = x_cat + residual  # tambah residual shortcut\n",
    "\n",
    "        # 5ï¸âƒ£ Dropout dan FC\n",
    "        x_res = self.dropout(x_res)\n",
    "        out = self.fc(x_res)\n",
    "        return out\n",
    "\n",
    "# ðŸ§  SE (Squeeze-and-Excitation) Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16): # Default = 16, amdhan = 4\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = torch.mean(x, dim=0, keepdim=True)  # Squeeze\n",
    "        w = F.relu(self.fc1(w))\n",
    "        w = torch.sigmoid(self.fc2(w))\n",
    "        return x * w  # Excitation\n",
    "\n",
    "# PERCOBAAN 2\n",
    "# ðŸ”§ TextCNN Enhanced (simplified)\n",
    "class TextCNNEnhanced(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_classes=2, output_dim=64, dropout_rate=0.3):\n",
    "        super(TextCNNEnhanced, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        # Depthwise + Pointwise convolution blocks\n",
    "        self.depthwise_conv1 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, dilation=1, groups=embed_dim, padding=1)\n",
    "        self.pointwise_conv1 = nn.Conv1d(embed_dim, output_dim, kernel_size=1)\n",
    "\n",
    "        self.depthwise_conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=4, dilation=2, groups=embed_dim, padding=3)\n",
    "        self.pointwise_conv2 = nn.Conv1d(embed_dim, output_dim, kernel_size=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.se_block = SEBlock(output_dim * 2)\n",
    "        self.fc = nn.Linear(output_dim * 2, num_classes)\n",
    "\n",
    "    # Helper block untuk konvolusi + aktivasi + pooling\n",
    "    def conv_block(self, x, depthwise, pointwise):\n",
    "        x = depthwise(x)\n",
    "        x = F.relu(pointwise(x))\n",
    "        x = F.max_pool1d(x, kernel_size=x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)            # (batch, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)           # (batch, embed_dim, seq_len)\n",
    "\n",
    "        # ðŸ”¸ Dua jalur konvolusi dengan cara yang sama\n",
    "        x1 = self.conv_block(x, self.depthwise_conv1, self.pointwise_conv1)\n",
    "        x2 = self.conv_block(x, self.depthwise_conv2, self.pointwise_conv2)\n",
    "\n",
    "        # ðŸ”¸ Gabung dan lanjut ke SE + FC\n",
    "        x_cat = torch.cat((x1, x2), dim=1)\n",
    "        x_cat = self.se_block(x_cat)\n",
    "        x_cat = self.dropout(x_cat)\n",
    "        out = self.fc(x_cat)\n",
    "        return out\n",
    "\n",
    "# PERCOBAAN 3\n",
    "# ðŸ”§ TextCNN Enhanced (simplified) + ResNorm\n",
    "class TextCNNEnhancedUltimate(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_classes=2, output_dim=64, dropout_rate=0.3):\n",
    "        super(TextCNNEnhancedUltimate, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        # Depthwise + Pointwise convolution blocks\n",
    "        self.depthwise_conv1 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, dilation=1, groups=embed_dim, padding=1)\n",
    "        self.pointwise_conv1 = nn.Conv1d(embed_dim, output_dim, kernel_size=1)\n",
    "\n",
    "        self.depthwise_conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=4, dilation=2, groups=embed_dim, padding=3)\n",
    "        self.pointwise_conv2 = nn.Conv1d(embed_dim, output_dim, kernel_size=1)\n",
    "\n",
    "        self.residual_proj = nn.Conv1d(embed_dim, output_dim, kernel_size=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.se_block = SEBlock(output_dim * 2)\n",
    "        self.fc = nn.Linear(output_dim * 2, num_classes)\n",
    "\n",
    "    # Helper block untuk konvolusi + aktivasi + pooling\n",
    "    def conv_block(self, x, depthwise, pointwise):\n",
    "        residual = x  # simpan input\n",
    "        x = depthwise(x)\n",
    "        x = F.relu(pointwise(x))\n",
    "        if residual.shape[1] != x.shape[1]: # Jika ukuran dimensi berbeda\n",
    "            residual = self.residual_proj(residual)\n",
    "        x = x + residual  # tambahkan residual\n",
    "        x = F.max_pool1d(x, kernel_size=x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x)            # (batch, seq_len, embed_dim)\n",
    "        x = x_embed.permute(0, 2, 1)           # (batch, embed_dim, seq_len)\n",
    "\n",
    "        # ðŸ”¸ Dua jalur konvolusi dengan cara yang sama\n",
    "        x1 = self.conv_block(x, self.depthwise_conv1, self.pointwise_conv1)\n",
    "        x2 = self.conv_block(x, self.depthwise_conv2, self.pointwise_conv2)\n",
    "\n",
    "        # ðŸ”¸ Gabung dan lanjut ke SE + FC\n",
    "        x_cat = torch.cat((x1, x2), dim=1)\n",
    "        x_cat = self.se_block(x_cat)\n",
    "        # residual = self.shortcut(x_embed.mean(dim=1))\n",
    "        # x_cat = x_cat + residual\n",
    "        x_cat = self.dropout(x_cat)\n",
    "        out = self.fc(x_cat)\n",
    "        return out\n",
    "\n",
    "class AmdhanTextCNNEnhanced(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_classes=2, output_dim=64, dropout_rate=0.3):\n",
    "        super(AmdhanTextCNNEnhanced, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embed_dim, output_dim, kernel_size=3)\n",
    "\n",
    "        # Depthwise + Pointwise convolution blocks\n",
    "        self.depthwise_conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=4, dilation=1, groups=embed_dim, padding=1)\n",
    "        self.pointwise_conv2 = nn.Conv1d(embed_dim, output_dim, kernel_size=1)\n",
    "\n",
    "        self.depthwise_conv3 = nn.Conv1d(embed_dim, embed_dim, kernel_size=5, dilation=2, groups=embed_dim, padding=3)\n",
    "        self.pointwise_conv3 = nn.Conv1d(embed_dim, output_dim, kernel_size=1)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.se_block = SEBlock(output_dim * 3)\n",
    "        self.fc = nn.Linear(output_dim * 3, num_classes)\n",
    "\n",
    "    # Helper block untuk konvolusi + aktivasi + pooling\n",
    "    def conv_block(self, x, depthwise, pointwise):\n",
    "        x = depthwise(x)\n",
    "        x = F.relu(pointwise(x))\n",
    "        x = F.max_pool1d(x, kernel_size=x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)            # (batch, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)           # (batch, embed_dim, seq_len)\n",
    "\n",
    "        # ðŸ”¸ Dua jalur konvolusi\n",
    "        x1 = F.relu(self.conv1(x)).max(dim=2)[0]\n",
    "        x2 = self.conv_block(x, self.depthwise_conv2, self.pointwise_conv2)\n",
    "        x3 = self.conv_block(x, self.depthwise_conv3, self.pointwise_conv3)\n",
    "\n",
    "        # ðŸ”¸ Gabung dan lanjut ke SE + FC\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)\n",
    "        x_cat = self.se_block(x_cat)\n",
    "        x_cat = self.dropout(x_cat)\n",
    "        out = self.fc(x_cat)\n",
    "        return out\n",
    "\n",
    "class MineTextCNNEnhanced(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_classes=2, output_dim=64, dropout_rate=0.3):\n",
    "        super(MineTextCNNEnhanced, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embed_dim, output_dim, kernel_size=3)\n",
    "\n",
    "        self.dilated_conv2 = nn.Conv1d(embed_dim, output_dim, kernel_size=5, dilation=2, padding=4)\n",
    "\n",
    "        self.depthwise_conv3 = nn.Conv1d(embed_dim, embed_dim, kernel_size=5, dilation=2, groups=embed_dim, padding=3)\n",
    "        self.pointwise_conv3 = nn.Conv1d(embed_dim, output_dim, kernel_size=1)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.se_block = SEBlock(output_dim * 3)\n",
    "        self.fc = nn.Linear(output_dim * 3, num_classes)\n",
    "\n",
    "    # Helper block untuk konvolusi + aktivasi + pooling\n",
    "    def conv_block(self, x, depthwise, pointwise):\n",
    "        x = depthwise(x)\n",
    "        x = F.relu(pointwise(x))\n",
    "        x = F.max_pool1d(x, kernel_size=x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)            # (batch, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)           # (batch, embed_dim, seq_len)\n",
    "\n",
    "        # ðŸ”¸ Dua jalur konvolusi\n",
    "        x1 = F.relu(self.conv1(x)).max(dim=2)[0]\n",
    "        x_dilated = F.relu(self.dilated_conv2(x))\n",
    "        x2 = F.max_pool1d(x_dilated, kernel_size=x_dilated.size(2)).squeeze(2)\n",
    "        x3 = self.conv_block(x, self.depthwise_conv3, self.pointwise_conv3)\n",
    "\n",
    "        # ðŸ”¸ Gabung dan lanjut ke SE + FC\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)\n",
    "        x_cat = self.se_block(x_cat)\n",
    "        x_cat = self.dropout(x_cat)\n",
    "        out = self.fc(x_cat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744781d-f8fa-4493-9e3c-c571c717b48c",
   "metadata": {},
   "source": [
    "## 4. TRAIN SECTION\n",
    "Bagian untuk train model yang sudah dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c678bdcf-a24e-46fc-bf0b-b7b3358baa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 29082002\n",
    "# Training Model\n",
    "DATASET_PATH = '../dataset/cyberbullying.csv'\n",
    "MODEL_OUTPUT_PATH = 'model_outputs'\n",
    "# K-fold Cross-validation\n",
    "N_FOLDS = 5\n",
    "MAX_LENGTH = 128\n",
    "# Training Model\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-3\n",
    "TOKENIZER_NAME = 'indobenchmark/indobert-base-p1'\n",
    "# CNN\n",
    "DROPOUT_RATE = 0.1\n",
    "NUM_CLASSES = 2\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_FILTERS = 100\n",
    "KERNEL_SIZE = [2, 4]\n",
    "OUT_CHANNELS = 50\n",
    "# Early Stopping\n",
    "PATIENCE = 5 # Patience for early stopping (epochs to wait after no improvement)\n",
    "MIN_DELTA = 0.001 # Minimum change in val_loss to be considered an improvement for early stopping\n",
    "# Action\n",
    "PYPLOT = False\n",
    "IMAGE_NAME = 'training_metrics.png'\n",
    "WANDB_NOTES = 'augmentasi dg probabilitas sama rata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "330d3211-869e-4413-9352-88057db53805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ploting_result(epochs, lr, train_loss, train_acc, train_f1, val_loss, val_acc, val_f1):\n",
    "    \"\"\"Ploting hasil train (untuk mode offline)\"\"\"    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Plot loss\n",
    "    axs[0].plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "    axs[0].plot(epochs, val_loss, label='Val loss', marker='s')\n",
    "    axs[0].set_title('Loss per Epoch')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    axs[1].plot(epochs, train_acc, label='Train Acc', marker='o')\n",
    "    axs[1].plot(epochs, val_acc, label='Val Acc', marker='s')\n",
    "    axs[1].set_title('Accuracy per Epoch')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot F1-score & Learning Rate\n",
    "    ax2 = axs[2].twinx()\n",
    "    \n",
    "    axs[2].plot(epochs, train_f1, label='Train F1-score', marker='o')\n",
    "    axs[2].plot(epochs, val_f1, label='Val F1-score', marker='s')\n",
    "    ax2.plot(epochs, lr,  color='orange', linestyle='--', label='Learning Rate', marker='x')\n",
    "    axs[2].set_title('F1-Score & Learning Rate')\n",
    "    axs[2].set_xlabel('Epoch')\n",
    "    axs[2].set_ylabel('F1-score')\n",
    "    ax2.set_ylabel('Learning Rate', color='orange')\n",
    "    axs[2].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Gabungkan Legend Kedua Axis\n",
    "    lns1, lbls1 = axs[2].get_legend_handles_labels()\n",
    "    lns2, lbls2 = ax2.get_legend_handles_labels()\n",
    "    axs[2].legend(lns1 + lns2, lbls1 + lbls2, loc='center right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Simpan File\n",
    "    plt.savefig(IMAGE_NAME, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Tampilkan di layar\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "fab7ae8e-ea48-44c7-b880-9bde8cb39419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_dataloaders_for_fold():\n",
    "    \"\"\"Create train and validation datasets/dataloaders for the fold\"\"\"\n",
    "    train_dataset = CyberbullyingDataset(\n",
    "        file_path=DATASET_PATH,\n",
    "        tokenizer_name=TOKENIZER_NAME,\n",
    "        random_state=SEED,\n",
    "        split=\"train\",\n",
    "        n_folds=N_FOLDS,\n",
    "        max_length=MAX_LENGTH,\n",
    "    ) # Membuat fold train dataset\n",
    "\n",
    "    val_dataset = CyberbullyingDataset(\n",
    "        file_path=DATASET_PATH,\n",
    "        tokenizer_name=TOKENIZER_NAME,\n",
    "        random_state=SEED,\n",
    "        split=\"val\",\n",
    "        n_folds=N_FOLDS,\n",
    "        max_length=MAX_LENGTH,\n",
    "    ) # Membuat fold val dataset\n",
    "    \n",
    "    # Membuat DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        # Jumlah epoch berturut-turut tanpa peningkatan signifikan yang masih ditoleransi\n",
    "        self.patience = patience\n",
    "        \n",
    "        # Selisih minimal antara val_loss sebelumnya dan sekarang yang dianggap sebagai perbaikan\n",
    "        self.min_delta = min_delta\n",
    "        \n",
    "        # Counter untuk menghitung berapa kali val_loss tidak membaik secara signifikan\n",
    "        self.counter = 0\n",
    "        \n",
    "        # Menyimpan nilai val_loss terbaik yang pernah dicapai\n",
    "        self.best_loss = float('inf')  # Awalnya di-set sangat besar\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        # Cek apakah val_loss sekarang lebih baik dari best_loss sebelumnya dengan selisih signifikan\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            # Jika ya, anggap ini sebagai peningkatan\n",
    "            self.best_loss = val_loss  # Perbarui best_loss\n",
    "            self.counter = 0           # Reset counter karena ada peningkatan\n",
    "        else:\n",
    "            # Jika tidak ada peningkatan signifikan, tambahkan counter\n",
    "            self.counter += 1\n",
    "\n",
    "        # Jika counter melebihi atau sama dengan batas patience, kembalikan True (hentikan training)\n",
    "        if self.counter >= self.patience:\n",
    "            return True\n",
    "\n",
    "        # Jika belum melebihi patience, teruskan training\n",
    "        return False\n",
    "\n",
    "def cnn_train_fold():\n",
    "    print(f\"\\n{'='*5} Fold {N_FOLDS+1} {'='*5}\")\n",
    "    \n",
    "    # Setup device\n",
    "    device = get_device()\n",
    "    \n",
    "    train_loader, val_loader = get_dataloaders_for_fold()\n",
    "\n",
    "    model = TextCNNEnhancedUltimate(\n",
    "        vocab_size=train_loader.dataset.vocab_size,\n",
    "        embed_dim=EMBEDDING_DIM,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        output_dim=OUT_CHANNELS,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "\n",
    "    # model = AmdhanTextCNNEnhanced(\n",
    "    #     vocab_size=train_loader.dataset.vocab_size,\n",
    "    #     embed_dim=EMBEDDING_DIM,\n",
    "    #     num_classes=NUM_CLASSES,\n",
    "    #     output_dim=OUT_CHANNELS,\n",
    "    #     dropout_rate=DROPOUT_RATE\n",
    "    # )\n",
    "\n",
    "    # model = MineTextCNNEnhanced(\n",
    "    #     vocab_size=train_loader.dataset.vocab_size,\n",
    "    #     embed_dim=EMBEDDING_DIM,\n",
    "    #     num_classes=NUM_CLASSES,\n",
    "    #     output_dim=OUT_CHANNELS,\n",
    "    #     dropout_rate=DROPOUT_RATE\n",
    "    # )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # hidden_weights = [p for p in model.parameters() if p.ndim >= 2]\n",
    "    # other_params = [p for p in model.parameters() if p.ndim < 2]\n",
    "\n",
    "    # param_groups = [\n",
    "    #     {\"params\": hidden_weights, \"use_muon\": True, \"lr\": 0.02, \"weight_decay\": 0.01},\n",
    "    #     {\"params\": other_params, \"use_muon\": False, \"lr\": 3e-4, \"betas\": (0.9, 0.95), \"weight_decay\": 0.01},\n",
    "    # ]\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) \n",
    "    # optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    # scheduler = ReduceLROnPlateau(\n",
    "    #     optimizer,       # optimizer yang mau diatur LR-nya\n",
    "    #     mode='max',      # 'max' kalau pakai metrik seperti F1 atau accuracy\n",
    "    #     factor=0.5,      # turunkan LR jadi setengah\n",
    "    #     patience=3,      # kalau 3 epoch berturut-turut val F1 tidak naik, LR diturunkan\n",
    "    #     verbose=True     # biar muncul log di output\n",
    "    # )\n",
    "\n",
    "    # ðŸ”½ Inisialisasi EarlyStopping\n",
    "    # early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA)\n",
    "\n",
    "    # PYPLOT\n",
    "    epochs = []\n",
    "    lr = []\n",
    "    train_loss_py = []\n",
    "    train_acc = []\n",
    "    train_f1 = []\n",
    "    val_loss_py = []\n",
    "    val_acc = []\n",
    "    val_f1 = []\n",
    "    # END\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        # PYPLOT\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        # END\n",
    "        \n",
    "        # Training loop\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False):\n",
    "            # Move batch to device\n",
    "            batch = to_device(batch, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch['input_ids'])\n",
    "            loss = criterion(outputs, batch['labels'])\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            train_total += batch['labels'].size(0)\n",
    "            train_correct += (predicted == batch['labels']).sum().item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            #PYPLOT\n",
    "            labels = batch['labels']\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            #END\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        # PYPLOT\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        # END\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                # Move batch to device\n",
    "                batch = to_device(batch, device)\n",
    "                \n",
    "                outputs = model(batch['input_ids'])\n",
    "                loss = criterion(outputs, batch['labels'])\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                val_total += batch['labels'].size(0)\n",
    "                val_correct += (predicted == batch['labels']).sum().item()\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                #PYPLOT\n",
    "                labels = batch['labels']\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                #END\n",
    "\n",
    "        # Print GPU memory usage if available\n",
    "        if torch.cuda.is_available():\n",
    "            memory_info = get_gpu_memory()\n",
    "            print(f\"GPU Memory Usage - Allocated: {memory_info['allocated']}, Cached: {memory_info['cached']}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        # PYPLOT\n",
    "        epochs.append(epoch+1)\n",
    "        lr.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        train_loss_py.append(avg_train_loss)\n",
    "        train_acc.append(train_accuracy)\n",
    "        train_f1.append(f1_score(train_labels, train_preds, average='weighted'))\n",
    "        \n",
    "        val_loss_py.append(avg_val_loss)\n",
    "        val_acc.append(val_accuracy)\n",
    "        val_f1.append(f1_score(val_labels, val_preds, average='weighted'))\n",
    "        # END\n",
    "        \n",
    "        # Log metrics\n",
    "        if PYPLOT == False:\n",
    "            wandb.log({\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"train_f1_score\": f1_score(train_labels, train_preds, average='weighted'),\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"val_f1_score\": f1_score(val_labels, val_preds, average='weighted'),\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                # \"epoch\": epoch + 1\n",
    "            })\n",
    "\n",
    "        scheduler.step()\n",
    "        # scheduler.step(f1)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # ðŸ”½ Early stopping check\n",
    "        # if early_stopping(avg_val_loss):\n",
    "        #     print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "        #     break\n",
    "\n",
    "    if PYPLOT == True:\n",
    "        ploting_result(epochs, lr, train_loss_py, train_acc, train_f1, val_loss_py, val_acc, val_f1)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the device to use (GPU if available, else CPU)\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move data to specified device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    return data\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get GPU memory usage if available\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            \"allocated\": f\"{torch.cuda.memory_allocated()/1e9:.2f} GB\",\n",
    "            \"cached\": f\"{torch.cuda.memory_reserved()/1e9:.2f} GB\"\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e7caf6e6-e534-4665-b104-ceff0630c48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (1.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\nikol\\OneDrive\\Tugas Akhir\\code\\wandb\\run-20251112_010559-q0vofjdr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/q0vofjdr' target=\"_blank\">exp_20251112_010559</a></strong> to <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying' target=\"_blank\">https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/q0vofjdr' target=\"_blank\">https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/q0vofjdr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 6 =====\n",
      "Using CPU\n",
      "Loading data from ../dataset/cyberbullying.csv...\n",
      "Menggunakan 5 folds dengan 1350 samples\n",
      "Loading data from ../dataset/cyberbullying.csv...\n",
      "Menggunakan 5 folds dengan 1350 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "Learning Rate: 0.005000\n",
      "Train Loss: 0.7068, Accuracy: 53.89%\n",
      "Val Loss: 0.5748, Accuracy: 70.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "Learning Rate: 0.005000\n",
      "Train Loss: 0.5379, Accuracy: 71.94%\n",
      "Val Loss: 0.4068, Accuracy: 81.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25\n",
      "Learning Rate: 0.005000\n",
      "Train Loss: 0.3350, Accuracy: 86.39%\n",
      "Val Loss: 0.3419, Accuracy: 88.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25\n",
      "Learning Rate: 0.005000\n",
      "Train Loss: 0.1782, Accuracy: 93.43%\n",
      "Val Loss: 0.3941, Accuracy: 85.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25\n",
      "Learning Rate: 0.005000\n",
      "Train Loss: 0.1385, Accuracy: 95.37%\n",
      "Val Loss: 0.3956, Accuracy: 88.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25\n",
      "Learning Rate: 0.005000\n",
      "Train Loss: 0.1246, Accuracy: 95.00%\n",
      "Val Loss: 0.4266, Accuracy: 87.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25\n",
      "Learning Rate: 0.005000\n",
      "Train Loss: 0.1181, Accuracy: 95.28%\n",
      "Val Loss: 0.4714, Accuracy: 87.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25\n",
      "Learning Rate: 0.005000\n",
      "Train Loss: 0.0922, Accuracy: 96.94%\n",
      "Val Loss: 0.4511, Accuracy: 88.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25\n",
      "Learning Rate: 0.005000\n",
      "Train Loss: 0.0927, Accuracy: 96.67%\n",
      "Val Loss: 0.4533, Accuracy: 88.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0939, Accuracy: 96.85%\n",
      "Val Loss: 0.4862, Accuracy: 86.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0707, Accuracy: 97.13%\n",
      "Val Loss: 0.4837, Accuracy: 87.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0867, Accuracy: 97.41%\n",
      "Val Loss: 0.4613, Accuracy: 87.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0769, Accuracy: 97.41%\n",
      "Val Loss: 0.3992, Accuracy: 88.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0649, Accuracy: 97.69%\n",
      "Val Loss: 0.3833, Accuracy: 88.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0582, Accuracy: 98.06%\n",
      "Val Loss: 0.3619, Accuracy: 88.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0649, Accuracy: 97.78%\n",
      "Val Loss: 0.4188, Accuracy: 88.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0570, Accuracy: 98.24%\n",
      "Val Loss: 0.4766, Accuracy: 88.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0587, Accuracy: 98.06%\n",
      "Val Loss: 0.4205, Accuracy: 89.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25\n",
      "Learning Rate: 0.002500\n",
      "Train Loss: 0.0466, Accuracy: 98.33%\n",
      "Val Loss: 0.4719, Accuracy: 88.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25\n",
      "Learning Rate: 0.001250\n",
      "Train Loss: 0.0620, Accuracy: 97.96%\n",
      "Val Loss: 0.4005, Accuracy: 89.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25\n",
      "Learning Rate: 0.001250\n",
      "Train Loss: 0.0429, Accuracy: 98.24%\n",
      "Val Loss: 0.3996, Accuracy: 89.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25\n",
      "Learning Rate: 0.001250\n",
      "Train Loss: 0.0450, Accuracy: 98.24%\n",
      "Val Loss: 0.4445, Accuracy: 86.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25\n",
      "Learning Rate: 0.001250\n",
      "Train Loss: 0.0529, Accuracy: 98.33%\n",
      "Val Loss: 0.3817, Accuracy: 89.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25\n",
      "Learning Rate: 0.001250\n",
      "Train Loss: 0.0362, Accuracy: 98.61%\n",
      "Val Loss: 0.4233, Accuracy: 88.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25\n",
      "Learning Rate: 0.001250\n",
      "Train Loss: 0.0573, Accuracy: 97.87%\n",
      "Val Loss: 0.4174, Accuracy: 88.52%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–</td></tr><tr><td>train_accuracy</td><td>â–â–„â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_f1_score</td><td>â–â–„â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_accuracy</td><td>â–â–…â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_f1_score</td><td>â–â–…â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–ƒâ–â–ƒâ–ƒâ–„â–…â–„â–„â–…â–…â–…â–ƒâ–‚â–‚â–ƒâ–…â–ƒâ–…â–ƒâ–ƒâ–„â–‚â–ƒâ–ƒ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.00125</td></tr><tr><td>train_accuracy</td><td>97.87037</td></tr><tr><td>train_f1_score</td><td>0.9787</td></tr><tr><td>train_loss</td><td>0.05731</td></tr><tr><td>val_accuracy</td><td>88.51852</td></tr><tr><td>val_f1_score</td><td>0.88499</td></tr><tr><td>val_loss</td><td>0.41742</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp_20251112_010559</strong> at: <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/q0vofjdr' target=\"_blank\">https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/q0vofjdr</a><br> View project at: <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying' target=\"_blank\">https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_010559-q0vofjdr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAIN MAIN FUNCTION\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "\n",
    "    if PYPLOT == False:\n",
    "        # Menggunakan WandB\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        wandb.init(\n",
    "            project=\"sentiment-analys-cyberbullying\",\n",
    "            name=f\"exp_{timestamp}\",\n",
    "            config={\n",
    "                # Data & tokenization\n",
    "                \"max_length\": MAX_LENGTH,\n",
    "                \"tokenizer\": TOKENIZER_NAME,\n",
    "        \n",
    "                # Model parameters\n",
    "                \"dropout\": DROPOUT_RATE,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"embed_dim\": EMBEDDING_DIM,\n",
    "                \"num_classes\": NUM_CLASSES,\n",
    "                \"num_filters\": NUM_FILTERS,\n",
    "                \"kernel_size\": KERNEL_SIZE,\n",
    "                \"out_channels\": OUT_CHANNELS,\n",
    "        \n",
    "                # Training parameters\n",
    "                \"n_folds\": N_FOLDS,\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "            },\n",
    "            notes=WANDB_NOTES\n",
    "        )\n",
    "    \n",
    "        # Train\n",
    "        cnn_train_fold()\n",
    "            \n",
    "        wandb.finish()\n",
    "\n",
    "    if PYPLOT == True:\n",
    "        cnn_train_fold()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd84f29-77a7-4793-90aa-cb84df472fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "ta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
