{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8499bbae-f3ea-47bf-a639-2aafa433b57e",
   "metadata": {},
   "source": [
    "## 1. Download Packages & Resource\n",
    "Download packages yang diperlukan untuk preprocessing, model ataupun training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d53e72f-898f-4897-80a9-7b9e46095370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR SOME SECTION\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# FOR PREPROCESSING SECTION\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# FOR MODEL SECTION\n",
    "from torch import nn # from torch import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# FOR TRAIN SECTION\n",
    "import wandb\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FOR EVALUATION SECTION\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW, Muon\n",
    "# from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd0d807d-d231-4931-bac9-9851ae84fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK yang diperlukan\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d65bd8-81d4-4744-b4cd-7861880f1afe",
   "metadata": {},
   "source": [
    "## 2. Dataset Class\n",
    "Kelas dari dataset yang mana dilakukan proses preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82320a68-87c0-4bd4-91ea-fff1a08098ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyberbullyingDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            file_path=\"../dataset/cyberbullying.csv\",\n",
    "            tokenizer_name=\"indobenchmark/indobert-base-p1\",\n",
    "            folds_file=\"k_folds.json\",\n",
    "            random_state=29082002,\n",
    "            split=\"train\",\n",
    "            fold=0,\n",
    "            n_folds=5,\n",
    "            max_length=128,\n",
    "    ):\n",
    "        self.file_path = file_path\n",
    "        self.folds_file = folds_file\n",
    "        self.random_state = random_state\n",
    "        self.split = split\n",
    "        self.fold = fold\n",
    "        self.n_folds = n_folds\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "\n",
    "        # Load dataset\n",
    "        self.load_data()\n",
    "        # Setup n-Fold Cross Validation\n",
    "        self.setup_folds()\n",
    "        # Mempersiapkan Indices (bentuk jamak index)\n",
    "        self.setup_indices()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Mengambil index dari data yang akan diambil\n",
    "        idx = self.indices[idx]\n",
    "        # Mengambil data komentar dan sentiment\n",
    "        text = str(self.df.iloc[idx][\"comment\"])\n",
    "        label = str(self.df.iloc[idx][\"sentiment\"])\n",
    "        # Melakukan Pre-Processing\n",
    "        comment_processed = self.preprocess(text)\n",
    "        # Tokenisasi\n",
    "        encoding = self.tokenizer(\n",
    "            comment_processed,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        data = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(int(label), dtype=torch.long),\n",
    "            'original_text': text,\n",
    "            'processed_text': comment_processed,\n",
    "            'original_index': idx\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def load_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "        \n",
    "        # Load dictionary slang dari file JSON\n",
    "        with open('slang_dictionary.json', 'r') as file:\n",
    "            slang_dict = json.load(file)\n",
    "\n",
    "    def aeda_augment(self, text):\n",
    "        \"\"\"\n",
    "        Melakukan augmentasi teks dengan metode AEDA:\n",
    "        Menyisipkan tanda baca secara acak di posisi acak dalam teks.\n",
    "        \"\"\"\n",
    "        punctuations = [\".\", \";\", \"?\", \":\", \"!\", \",\"]\n",
    "        words = text.split()\n",
    "        if len(words) == 0:\n",
    "            return text\n",
    "    \n",
    "        # # Tentukan berapa banyak tanda baca yang akan disisipkan\n",
    "        # n_insert = random.randint(1, max(1, len(words) // 3))\n",
    "        \n",
    "        # # Pilih posisi acak untuk sisipan\n",
    "        # positions = random.sample(range(len(words)), n_insert)\n",
    "        # positions.sort(reverse=True)  # disisipkan dari belakang biar indeks tidak bergeser\n",
    "    \n",
    "        # for pos in positions:\n",
    "        #     punct = random.choice(punctuations)\n",
    "        #     words.insert(pos, punct)\n",
    "\n",
    "        # Pilih posisi acak untuk sisipan\n",
    "        position = random.randint(0, len(words) - 1)\n",
    "        \n",
    "        # Pilih tanda baca acak\n",
    "        punct = random.choice(punctuations)\n",
    "        \n",
    "        # Sisipkan tanda baca ke dalam list kata\n",
    "        words.insert(position, punct)\n",
    "    \n",
    "        return \" \".join(words)\n",
    "\n",
    "    def random_typo(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) < 1:\n",
    "            return text\n",
    "        # Pilih satu kata secara acak untuk dimodifikasi\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        word = words[idx]\n",
    "        if len(word) > 1:\n",
    "            char_list = list(word)\n",
    "            # Pilih posisi acak untuk swap\n",
    "            i = random.randint(0, len(char_list) - 2)\n",
    "            # swap 2 huruf berdekatan\n",
    "            char_list[i], char_list[i+1] = char_list[i+1], char_list[i] \n",
    "            words[idx] = ''.join(char_list)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def random_swap(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            return text\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def random_delete(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) <= 1:\n",
    "            return text\n",
    "        # Pilih satu kata secara acak untuk dihapus\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        # Hapus kata tersebut\n",
    "        del words[idx]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def augmentation_text(self, text, probability=0.5):\n",
    "        # Hanya lakukan augmentasi dengan probabilitas tertentu\n",
    "        if random.random() > probability:\n",
    "            return text\n",
    "\n",
    "        # PROBABILITAS ACAK\n",
    "        # Daftar semua fungsi augmentasi yang tersedia\n",
    "        augmentations = [\n",
    "            self.aeda_augment,\n",
    "            self.random_typo,\n",
    "            self.random_swap,\n",
    "            self.random_delete\n",
    "        ]\n",
    "        \n",
    "        # Pecah kalimat menjadi kumpulan kata\n",
    "        words = text.split()\n",
    "        # Tentukan berapa banyak augmentasi yang akan dilakukan\n",
    "        n_insert = random.randint(1, max(1, len(words) // 3))\n",
    "        for i in range(n_insert):\n",
    "            # Pilih satu augmentasi secara acak\n",
    "            augmentation_func = random.choice(augmentations)\n",
    "            # Terapkan augmentasi yang dipilih\n",
    "            text = augmentation_func(text)\n",
    "        return text\n",
    "\n",
    "        # # Pilih satu augmentasi secara acak\n",
    "        # augmentation_func = random.choice(augmentations)\n",
    "        # # Mengembalikan augmentasi yang dipilih\n",
    "        # return augmentation_func(text)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Konversi ke huruf kecil\n",
    "        text = text.lower()\n",
    "\n",
    "        # Hapus mention (@) dan hashtag (#)\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "        # # Hapus emoji dan karakter non-ASCII\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "        # Augmentasi\n",
    "        text = self.augmentation_text(text)\n",
    "        \n",
    "        # Tokenisasi\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Menghapus stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Menggabungkan kembali kata-kata menjadi kalimat\n",
    "        text = ' '.join(words)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def setup_indices(self):\n",
    "        '''\n",
    "        Mempersiapkan indices untuk data yang akan di-training\n",
    "        '''\n",
    "        fold_key = f\"fold_{self.fold}\"\n",
    "        if self.split == \"train\":\n",
    "            self.indices = self.fold_indices[fold_key]['train_indices']\n",
    "        else:\n",
    "            self.indices = self.fold_indices[fold_key]['val_indices']\n",
    "\n",
    "    def setup_folds(self):\n",
    "        # Jika fold sudah ada, maka load fold\n",
    "        if os.path.exists(self.folds_file):\n",
    "            self.load_folds()\n",
    "        # Jika tidak ada, maka buat fold\n",
    "        else:\n",
    "            self.create_folds()\n",
    "\n",
    "    def load_folds(self):\n",
    "        '''\n",
    "        Apabila fold sudah ada, maka load fold\n",
    "        '''\n",
    "        with open(self.folds_file, 'r') as f:\n",
    "            fold_data = json.load(f)\n",
    "        self.fold_indices = fold_data['fold_indices']\n",
    "        print(f\"Menggunakan {fold_data['n_folds']} folds dengan {fold_data['n_samples']} samples\")\n",
    "    \n",
    "    def create_folds(self):\n",
    "        '''\n",
    "        Apabila fold sudah ada, maka load fold\n",
    "        '''\n",
    "        print(f\"Membuat n-fold CV dengan random state {self.random_state}\")\n",
    "        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "        # print(\"\\nStratified k-fold positif samples per fold:\")\n",
    "        # for _, val_idx in skf.split(self.df, self.df['sentiment']):\n",
    "        #     print(f\"{np.sum(self.df['sentiment'].iloc[val_idx] == 1)} out of {len(val_idx)}\")\n",
    "\n",
    "        fold_indices = {}\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(self.df, self.df['sentiment'])):\n",
    "            fold_indices[f\"fold_{fold}\"] = {\n",
    "                'train_indices': train_idx.tolist(),\n",
    "                'val_indices': val_idx.tolist()\n",
    "            }\n",
    "        \n",
    "        # Simpan fold ke file\n",
    "        with open(self.folds_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'fold_indices': fold_indices,\n",
    "                'n_samples': len(self.df),\n",
    "                'n_folds': self.n_folds,\n",
    "                'random_state': self.random_state\n",
    "            }, f)\n",
    "\n",
    "            self.fold_indices = fold_indices\n",
    "            print(f'Created {self.n_folds}-fold indices and saved to {self.folds_file}')\n",
    "\n",
    "    def load_data(self):\n",
    "        print(f'Loading data from {self.file_path}...')\n",
    "        self.df = pd.read_csv(self.file_path) # Load csv\n",
    "        # self.df.columns = ['sentiment', 'comment'] # Rename columns\n",
    "        if len(self.df.columns) == 2:\n",
    "            self.df.columns = ['sentiment', 'comment']\n",
    "        else:\n",
    "            print(\"âš ï¸ Jumlah kolom tidak sesuai, kolom asli:\", self.df.columns)\n",
    "        self.df = self.df.dropna(subset=['sentiment', 'comment']) # Drop NaN values\n",
    "        self.df['sentiment'] = self.df['sentiment'].astype(int) # Convert sentiment to int\n",
    "        self.df['sentiment'] = self.df['sentiment'].apply(lambda x: 1 if x == -1 else 0) # Transform labels: convert -1 to 1, and 1 to 0\n",
    "        self.df = self.df[(self.df['sentiment'] == 0) | (self.df['sentiment'] == 1)] # Filter sentiment\n",
    "\n",
    "        # Undersampling menyeimbangkan dataset (hanya untuk split \"train\")\n",
    "        if self.split == \"train\":\n",
    "            df_label_0 = self.df[self.df['sentiment'] == 0]\n",
    "            df_label_1 = self.df[self.df['sentiment'] == 1]\n",
    "\n",
    "            min_samples_per_class = min(len(df_label_0), len(df_label_1))\n",
    "\n",
    "            df_label_0_undersampled = df_label_0.sample(n=min_samples_per_class, random_state=self.random_state)\n",
    "            df_label_1_undersampled = df_label_1.sample(n=min_samples_per_class, random_state=self.random_state)\n",
    "\n",
    "            self.df = pd.concat([df_label_0_undersampled, df_label_1_undersampled])\n",
    "            self.df = self.df.sample(frac=1, random_state=self.random_state).reset_index(drop=True)\n",
    "\n",
    "        # df_label_0 = self.df[self.df['sentiment'] == 0]\n",
    "        # df_label_1 = self.df[self.df['sentiment'] == 1]\n",
    "\n",
    "        # min_samples_per_class = min(len(df_label_0), len(df_label_1))\n",
    "\n",
    "        # df_label_0_undersampled = df_label_0.sample(n=min_samples_per_class, random_state=self.random_state)\n",
    "        # df_label_1_undersampled = df_label_1.sample(n=min_samples_per_class, random_state=self.random_state)\n",
    "\n",
    "        # self.df = pd.concat([df_label_0_undersampled, df_label_1_undersampled])\n",
    "        # self.df = self.df.sample(frac=1, random_state=self.random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f8440-e9fb-45db-8db8-abdf16a2d8c0",
   "metadata": {},
   "source": [
    "### 2x. Main Section Preprocessing\n",
    "Bagian utama untuk menjalankan program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d60e57-bdbd-438a-af17-5f3416a6b252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     dataset = CyberbullyingDataset(fold=0, split=\"train\")\n",
    "#     data = dataset[0]\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bb4fe-c97f-4469-bb36-c55ccbdce845",
   "metadata": {},
   "source": [
    "## 3. Model Class\n",
    "Kelas dari model machine learning yang akan di training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b006a7f5-3871-44e1-a3e2-1d8b7e8a80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1DFlat (Muon-compatible)    \n",
    "class Conv1DFlat(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # 2D parameter (Muon-compatible)\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels * kernel_size)\n",
    "        )\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # inisialisasi mirip Conv1D\n",
    "        nn.init.kaiming_uniform_(self.weight, a=5**0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, in_channels, seq_len)\n",
    "        weight_3d = self.weight.view(\n",
    "            self.out_channels,\n",
    "            self.in_channels,\n",
    "            self.kernel_size\n",
    "        )\n",
    "        return F.conv1d(x, weight_3d, self.bias)\n",
    "\n",
    "# TextCNN\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        in_channels=300,\n",
    "        num_classes=2,\n",
    "        conv_filters=100,\n",
    "        kernel_sizes=[3, 4, 5],\n",
    "        dropout_rate=0.5\n",
    "    ):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, in_channels)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels, conv_filters, kernel_size=k)\n",
    "            # Conv1DFlat(in_channels, conv_filters, kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(conv_filters * len(kernel_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        x = self.embedding(x)          # (batch, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)         # (batch, embed_dim, seq_len)\n",
    "\n",
    "        conv_outs = []\n",
    "        for conv in self.convs:\n",
    "            h = F.relu(conv(x))        # (batch, conv_filters, L')\n",
    "            h = torch.max(h, dim=2)[0]\n",
    "            conv_outs.append(h)\n",
    "\n",
    "        x = torch.cat(conv_outs, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ğŸ”§ TextCNN ResNorm\n",
    "class ResidualTextCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    ResidualTextCNN\n",
    "    - Gabungan TextCNNLight + BatchNorm + Residual connection\n",
    "    - Stabil, ringan, cocok untuk teks pendek seperti komentar media sosial.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, in_channels=100, num_classes=2, conv_filters=100, dropout_rate=0.5):\n",
    "        super(ResidualTextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, in_channels, padding_idx=0)\n",
    "\n",
    "        # Dua layer konvolusi dengan ukuran kernel berbeda\n",
    "        self.conv1 = nn.Conv1d(in_channels, conv_filters, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels, conv_filters, kernel_size=4, padding=1)\n",
    "\n",
    "        # Normalisasi batch setelah konvolusi\n",
    "        self.bn1 = nn.BatchNorm1d(conv_filters)\n",
    "        self.bn2 = nn.BatchNorm1d(conv_filters)\n",
    "\n",
    "        # Shortcut projection agar dimensi sama (residual)\n",
    "        self.shortcut = nn.Linear(in_channels, conv_filters * 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(conv_filters * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1ï¸âƒ£ Embedding\n",
    "        x_embed = self.embedding(x)  # (batch, seq_len, in_channels)\n",
    "        x_embed_t = x_embed.permute(0, 2, 1)  # (batch, in_channels, seq_len)\n",
    "\n",
    "        # 2ï¸âƒ£ Convolution + ReLU + BatchNorm + Global Max Pooling\n",
    "        x1 = F.relu(self.bn1(self.conv1(x_embed_t))).max(dim=2)[0]\n",
    "        x2 = F.relu(self.bn2(self.conv2(x_embed_t))).max(dim=2)[0]\n",
    "\n",
    "        # 3ï¸âƒ£ Concatenate hasil konvolusi\n",
    "        x_cat = torch.cat((x1, x2), dim=1)  # (batch, conv_filters * 2)\n",
    "\n",
    "        # 4ï¸âƒ£ Residual connection: proyeksikan embedding ke dimensi yang sama\n",
    "        residual = self.shortcut(x_embed.mean(dim=1))  # rata-rata embedding â†’ dim (conv_filters*2)\n",
    "        x_res = x_cat + residual  # tambah residual shortcut\n",
    "\n",
    "        # 5ï¸âƒ£ Dropout dan FC\n",
    "        x_res = self.dropout(x_res)\n",
    "        out = self.fc(x_res)\n",
    "        return out\n",
    "\n",
    "# SE (Squeeze-and-Excitation) Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels)\n",
    "        \n",
    "        # SQUEEZE:\n",
    "        # Pada TextCNN, dimensi temporal sudah dihilangkan\n",
    "        # oleh global max pooling sebelumnya.\n",
    "        # Oleh karena itu, tidak diperlukan lagi operasi\n",
    "        # pooling tambahan (mean / avg).\n",
    "        # Representasi vektor ini sudah merupakan ringkasan\n",
    "        # global dari tiap channel untuk satu sampel.\n",
    "        \n",
    "        # EXCITATION:\n",
    "        # Dua fully connected layer digunakan untuk\n",
    "        # mempelajari dependensi antar channel dan\n",
    "        # menghasilkan bobot pentingnya masing-masing channel.\n",
    "        # Sigmoid memastikan bobot berada pada rentang [0, 1].\n",
    "        w = F.relu(self.fc1(x))\n",
    "        w = torch.sigmoid(self.fc2(w))\n",
    "        \n",
    "        # Channel-wise reweighting:\n",
    "        # Setiap channel diperkuat atau dilemahkan\n",
    "        # secara adaptif berdasarkan konteks input.\n",
    "        return x * w\n",
    "\n",
    "# SEDepthwise TextCNN\n",
    "class SEDepthwiseTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, in_channels=300, num_classes=2, conv_filters=100, kernel_sizes=[3,4,5], dropout_rate=0.5):\n",
    "        super(SEDepthwiseTextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, in_channels, padding_idx=0)\n",
    "\n",
    "        # Depthwise + Pointwise convolution blocks\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels, in_channels, kernel_size=ks, groups=in_channels, padding=0),  # depthwise\n",
    "                nn.Conv1d(in_channels, conv_filters, kernel_size=1),  # pointwise\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            for ks in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.se_block = SEBlock(conv_filters * len(kernel_sizes))\n",
    "        self.fc = nn.Linear(conv_filters * len(kernel_sizes), num_classes)\n",
    "\n",
    "    # Helper block untuk konvolusi + aktivasi + pooling\n",
    "    def conv_block(self, x, depthwise, pointwise):\n",
    "        x = depthwise(x)\n",
    "        x = F.relu(pointwise(x))\n",
    "        x = F.max_pool1d(x, kernel_size=x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)   # (batch, seq_len, in_channels)\n",
    "        x = x.permute(0, 2, 1)  # (batch, in_channels, seq_len)\n",
    "\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            y = conv(x)\n",
    "            y = F.max_pool1d(y, kernel_size=y.size(2)).squeeze(2)\n",
    "            conv_outputs.append(y)\n",
    "\n",
    "        x_cat = torch.cat((conv_outputs), dim=1)    # (batch, conv_filters * 3)\n",
    "        x_cat = self.se_block(x_cat)\n",
    "        x_cat = self.dropout(x_cat)\n",
    "\n",
    "        return self.fc(x_cat)\n",
    "        \n",
    "# TextCNN Enhanced\n",
    "class EnhancedTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, in_channels=100, num_classes=2, conv_filters=100, dropout_rate=0.5):\n",
    "        super(EnhancedTextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, in_channels, padding_idx=0)\n",
    "\n",
    "        # Depthwise + Pointwise convolution blocks\n",
    "        self.depthwise_conv1 = nn.Conv1d(in_channels, in_channels, kernel_size=3, dilation=1, groups=in_channels, padding=1)\n",
    "        self.pointwise_conv1 = nn.Conv1d(in_channels, conv_filters, kernel_size=1)\n",
    "\n",
    "        self.depthwise_conv2 = nn.Conv1d(in_channels, in_channels, kernel_size=4, dilation=2, groups=in_channels, padding=3)\n",
    "        self.pointwise_conv2 = nn.Conv1d(in_channels, conv_filters, kernel_size=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.se_block = SEBlock(conv_filters * 2)\n",
    "        self.fc = nn.Linear(conv_filters * 2, num_classes)\n",
    "\n",
    "    # Helper block untuk konvolusi + aktivasi + pooling\n",
    "    def conv_block(self, x, depthwise, pointwise):\n",
    "        x = depthwise(x)\n",
    "        x = F.relu(pointwise(x))\n",
    "        x = F.max_pool1d(x, kernel_size=x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)            # (batch, seq_len, in_channels)\n",
    "        x = x.permute(0, 2, 1)           # (batch, in_channels, seq_len)\n",
    "\n",
    "        # ğŸ”¸ Dua jalur konvolusi dengan cara yang sama\n",
    "        x1 = self.conv_block(x, self.depthwise_conv1, self.pointwise_conv1)\n",
    "        x2 = self.conv_block(x, self.depthwise_conv2, self.pointwise_conv2)\n",
    "\n",
    "        # ğŸ”¸ Gabung dan lanjut ke SE + FC\n",
    "        x_cat = torch.cat((x1, x2), dim=1)\n",
    "        x_cat = self.se_block(x_cat)\n",
    "        x_cat = self.dropout(x_cat)\n",
    "        out = self.fc(x_cat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744781d-f8fa-4493-9e3c-c571c717b48c",
   "metadata": {},
   "source": [
    "## 4. TRAIN SECTION\n",
    "Bagian untuk train model yang sudah dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c678bdcf-a24e-46fc-bf0b-b7b3358baa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 29082002\n",
    "# Training Model\n",
    "DATASET_PATH = '../dataset/cyberbullying.csv'\n",
    "MODEL_OUTPUT_PATH = 'model_outputs'\n",
    "# K-fold Cross-validation\n",
    "N_FOLDS = 5\n",
    "MAX_LENGTH = 128\n",
    "# Training Model\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 50 # ablation: (25, @50, 100)\n",
    "LEARNING_RATE = 5e-4 # ablation: (5e-2, @5e-3, 5e-4)\n",
    "TOKENIZER_NAME = 'indobenchmark/indobert-base-p1'\n",
    "OPTIMIZER_NAME = 'Muon' # ablation: (AdamW, Muon)\n",
    "# Model Hyperparameters\n",
    "NUM_CLASSES = 2\n",
    "EMBEDDING_DIM = 100 # light=100, medium=@300, large=600 => ablation: (50 , 100, 200)\n",
    "KERNEL_SIZE = [3, 4] # light=[3, 4], medium=@[3, 4, 5], large=[3, 4, 5, 6] => ablation: (@[3], [3, 4], @[3,4,5])\n",
    "CONV_FILTERS = 100 # light=50, medium=@100, large=200 => ablation: (25, 50, @100)\n",
    "DROPOUT_RATE = 0.5 # ablation: (0.4, @0.5, 0.6)\n",
    "# Early Stopping\n",
    "PATIENCE = 10\n",
    "# Notebook\n",
    "IS_NOTEBOOK = True\n",
    "# WANDB_NOTE = 'Hyperparameter baseline untuk TextCNN SEDepthwise'\n",
    "WANDB_NOTE = 'Light TextCNN best hyperparameter tuning'\n",
    "WANDB_GROUP = 'Light TextCNN best hyperparameter tuning'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d646a690",
   "metadata": {},
   "source": [
    "### 4.1. For One Fold\n",
    "Untuk train 1 fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fab7ae8e-ea48-44c7-b880-9bde8cb39419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Train CNN for sentiment analysis')\n",
    "\n",
    "    # Seed and reproducibility\n",
    "    parser.add_argument('--seed', type=int, default=SEED,\n",
    "                        help='Random seed for reproducibility')\n",
    "    \n",
    "    # Data parameters\n",
    "    parser.add_argument('--dataset_path', type=str, default=DATASET_PATH, \n",
    "                        help='Path to dataset file')\n",
    "    parser.add_argument('--max_length', type=int, default=MAX_LENGTH,\n",
    "                        help='Maximum sequence length')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--tokenizer', type=str, default=TOKENIZER_NAME,\n",
    "                        help='Tokenizer name')\n",
    "    parser.add_argument('--dropout', type=float, default=DROPOUT_RATE,\n",
    "                        help='Dropout rate')\n",
    "    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE,\n",
    "                        help='Batch size for embedding')\n",
    "    parser.add_argument('--embed_dim', type=int, default=EMBEDDING_DIM, \n",
    "                        help='Embedding dimension for CNN')\n",
    "    parser.add_argument('--num_classes', type=int, default=NUM_CLASSES, \n",
    "                        help='Number of classes')\n",
    "    parser.add_argument('--conv_filters', type=int, default=CONV_FILTERS, \n",
    "                        help='Number of filters for CNN')\n",
    "    parser.add_argument('--kernel_size', type=int, default=KERNEL_SIZE, \n",
    "                        help='Kernel sizes for CNN')\n",
    "\n",
    "    # Training parameters\n",
    "    parser.add_argument('--n_folds', type=int, default=N_FOLDS,\n",
    "                        help='Fold number for cross-validation')\n",
    "    parser.add_argument('--epochs', type=int, default=EPOCHS,\n",
    "                        help='Number of epochs')\n",
    "    parser.add_argument('--lr', type=float, default=LEARNING_RATE,\n",
    "                        help='Learning rate')\n",
    "    \n",
    "    # Output parameters\n",
    "    parser.add_argument('--output_model', action='store_false', default=True,\n",
    "                       help='Save model after training')\n",
    "    \n",
    "    parser.add_argument('--output_dir', type=str, default=MODEL_OUTPUT_PATH,\n",
    "                        help='Directory to save model outputs')\n",
    "\n",
    "    # Wandb parameters\n",
    "    parser.add_argument('--use_wandb', action='store_true', default=False,\n",
    "                       help='Enable Weights & Biases logging')\n",
    "    parser.add_argument('--wandb_group', type=str, default=WANDB_GROUP,\n",
    "                    help='Create group for Weights & Biases runs')\n",
    "    parser.add_argument('--wandb_note', type=str, default=WANDB_NOTE,\n",
    "                        help='Add Weights & Biases notes')\n",
    "    \n",
    "    # Early Stopping parameters\n",
    "    parser.add_argument('--patience', type=int, default=5,\n",
    "                        help='Patience for early stopping (epochs to wait after no improvement)')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def create_output_dir(base_dir):\n",
    "    \"\"\"Create timestamped output directory\"\"\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(base_dir, f\"run_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def get_dataloaders_for_fold(args, fold=0):\n",
    "    \"\"\"Create train and validation datasets/dataloaders for the fold\"\"\"\n",
    "    # Membuat fold train dataset\n",
    "    train_dataset = CyberbullyingDataset(\n",
    "        file_path=args.dataset_path,\n",
    "        tokenizer_name=args.tokenizer,\n",
    "        random_state=args.seed,\n",
    "        split=\"train\",\n",
    "        n_folds=args.n_folds,\n",
    "        fold=fold,\n",
    "        max_length=args.max_length,\n",
    "    )\n",
    "\n",
    "    # Membuat fold val dataset\n",
    "    val_dataset = CyberbullyingDataset(\n",
    "        file_path=args.dataset_path,\n",
    "        tokenizer_name=args.tokenizer,\n",
    "        random_state=args.seed,\n",
    "        split=\"val\",\n",
    "        n_folds=args.n_folds,\n",
    "        max_length=args.max_length,\n",
    "    )\n",
    "    \n",
    "    # Membuat DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def save_conf_matrix(true_labels, pred_labels):\n",
    "    df = pd.DataFrame({\n",
    "        \"y_true\": true_labels,\n",
    "        \"x_pred\": pred_labels\n",
    "    })\n",
    "\n",
    "    cm = confusion_matrix(df['y_true'], df['x_pred'])\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"Actual 0\", \"Actual 1\"],\n",
    "        columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "    sns.heatmap(\n",
    "        cm_df,\n",
    "        # annot=labels,\n",
    "        annot=True,\n",
    "        fmt=\"\",\n",
    "        cmap=\"Blues\",\n",
    "        cbar=True,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "    fig.savefig(\"confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "def create_model(args, train_loader, model_name=\"default-textcnn\"):\n",
    "    if model_name == 'default-textcnn':\n",
    "        model = TextCNN(\n",
    "            vocab_size=train_loader.dataset.vocab_size,\n",
    "            in_channels=args.embed_dim,\n",
    "            conv_filters=args.conv_filters,\n",
    "            kernel_sizes=args.kernel_size,\n",
    "            dropout_rate=args.dropout\n",
    "        )\n",
    "        \n",
    "    elif model_name == 'light-textcnn':\n",
    "        # print(\"Model vocabulary size:\", train_loader.dataset.vocab_size)\n",
    "        model = TextCNN(\n",
    "            vocab_size=train_loader.dataset.vocab_size,\n",
    "            in_channels=100,\n",
    "            conv_filters=50,\n",
    "            kernel_sizes=[3, 4],\n",
    "            dropout_rate=0.5\n",
    "        )\n",
    "\n",
    "    elif model_name == 'medium-textcnn':\n",
    "        model = TextCNN(\n",
    "            vocab_size=train_loader.dataset.vocab_size,\n",
    "            in_channels=300,\n",
    "            conv_filters=100,\n",
    "            kernel_sizes=[3, 4, 5],\n",
    "            dropout_rate=0.5\n",
    "        )\n",
    "\n",
    "    elif model_name == 'weight-textcnn':\n",
    "        model = TextCNN(\n",
    "            vocab_size=train_loader.dataset.vocab_size,\n",
    "            in_channels=600,\n",
    "            conv_filters=200,\n",
    "            kernel_sizes=[3, 4, 5, 6],\n",
    "            dropout_rate=0.5\n",
    "        )\n",
    "\n",
    "    elif model_name == 'residual-textcnn':\n",
    "        model = ResidualTextCNN(\n",
    "            vocab_size=train_loader.dataset.vocab_size,\n",
    "            in_channels=args.embed_dim,\n",
    "            conv_filters=args.conv_filters,\n",
    "            dropout_rate=args.dropout\n",
    "        )\n",
    "\n",
    "    elif model_name == 'sedepthwise-textcnn':\n",
    "        model = SEDepthwiseTextCNN(\n",
    "            vocab_size=train_loader.dataset.vocab_size,\n",
    "            in_channels=args.embed_dim,\n",
    "            conv_filters=args.conv_filters,\n",
    "            kernel_sizes=args.kernel_size,\n",
    "            dropout_rate=args.dropout\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    labels_all, preds_all = [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        batch = to_device(batch, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch['input_ids'])\n",
    "        loss = criterion(outputs, batch['labels'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total += batch['labels'].size(0)\n",
    "        correct += (preds == batch['labels']).sum().item()\n",
    "\n",
    "        labels_all.extend(batch['labels'].cpu().tolist())\n",
    "        preds_all.extend(preds.cpu().tolist())\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(loader),\n",
    "        \"accuracy\": 100 * correct / total,\n",
    "        \"labels\": labels_all,\n",
    "        \"preds\": preds_all\n",
    "    }\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    labels_all, preds_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            batch = to_device(batch, device)\n",
    "            outputs = model(batch['input_ids'])\n",
    "            loss = criterion(outputs, batch['labels'])\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total += batch['labels'].size(0)\n",
    "            correct += (preds == batch['labels']).sum().item()\n",
    "\n",
    "            labels_all.extend(batch['labels'].cpu().tolist())\n",
    "            preds_all.extend(preds.cpu().tolist())\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(loader),\n",
    "        \"accuracy\": 100 * correct / total,\n",
    "        \"labels\": labels_all,\n",
    "        \"preds\": preds_all\n",
    "    }\n",
    "\n",
    "def compute_metrics(labels, preds):\n",
    "    return {\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "        # \"precision\": precision_score(labels, preds, average='weighted', zero_division=0),\n",
    "        # \"recall\": recall_score(labels, preds, average='weighted', zero_division=0),\n",
    "        # \"f1\": f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "    }\n",
    "\n",
    "def cnn_train(args, output_dir=\"none\", cur_fold=0, model_name='light'):\n",
    "    print(f\"\\n{'='*5} Fold {cur_fold} {'='*5}\")\n",
    "\n",
    "    device = get_device()\n",
    "    train_loader, val_loader = get_dataloaders_for_fold(args, cur_fold)\n",
    "\n",
    "    model = create_model(args, train_loader, model_name).to(device)\n",
    "    if OPTIMIZER_NAME == 'AdamW':\n",
    "        optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "    elif OPTIMIZER_NAME == 'Muon':\n",
    "        optimizer = Muon(\n",
    "            [p for p in model.parameters() if p.dim() == 2],\n",
    "            lr=args.lr\n",
    "        )\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    # scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_labels, best_val_preds = None, None\n",
    "    best_epoch = None\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        train_out = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_out = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        train_metrics = compute_metrics(train_out[\"labels\"], train_out[\"preds\"])\n",
    "        val_metrics = compute_metrics(val_out[\"labels\"], val_out[\"preds\"])\n",
    "\n",
    "        # Siapkan dictionary log dasar\n",
    "        log_dict = {\n",
    "            \"fold\": cur_fold,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_out[\"loss\"],\n",
    "            \"train_accuracy\": train_out[\"accuracy\"],\n",
    "            \"train_precision\": train_metrics[\"precision\"],\n",
    "            \"train_recall\": train_metrics[\"recall\"],\n",
    "            \"train_f1\": train_metrics[\"f1\"],\n",
    "            \"val_loss\": val_out[\"loss\"],\n",
    "            \"val_accuracy\": val_out[\"accuracy\"],\n",
    "            \"val_precision\": val_metrics[\"precision\"],\n",
    "            \"val_recall\": val_metrics[\"recall\"],\n",
    "            \"val_f1\": val_metrics[\"f1\"],\n",
    "        }\n",
    "\n",
    "        # Logika Best Model & Confusion Matrix\n",
    "        if val_out[\"loss\"] < best_val_loss:\n",
    "            best_val_loss = val_out[\"loss\"]\n",
    "            best_epoch = epoch\n",
    "            best_val_preds = val_out[\"preds\"]\n",
    "            best_val_labels = val_out[\"labels\"]\n",
    "            \n",
    "            # Tambahkan Confusion Matrix ke dictionary log yang SAMA\n",
    "            if args.use_wandb or IS_NOTEBOOK:\n",
    "                log_dict[\"val_confusion_matrix\"] = wandb.plot.confusion_matrix(\n",
    "                    y_true=val_out[\"labels\"],\n",
    "                    preds=val_out[\"preds\"],\n",
    "                    class_names=[\"non_cyberbullying\", \"cyberbullying\"]\n",
    "                )\n",
    "                print(f\"Logged confusion matrix for fold {cur_fold} at epoch {epoch+1}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "        print(f\"Train Loss: {train_out['loss']:.4f} | Acc: {train_out['accuracy']:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_out['loss']:.4f} | Acc: {val_out['accuracy']:.2f}%\")\n",
    "        \n",
    "        # EKSEKUSI LOG WANDB (Hanya 1 kali per epoch)\n",
    "        if args.use_wandb or IS_NOTEBOOK:\n",
    "            wandb.log(log_dict)\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        # Early Stopping check\n",
    "        if early_stopping(val_out[\"loss\"]):\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        if args.output_model:\n",
    "            model_save_path = os.path.join(output_dir, f\"fold_{cur_fold+1}_model.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        \n",
    "    # save_conf_matrix(best_val_labels, best_val_preds)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the device to use (GPU if available, else CPU)\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move data to specified device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    return data\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get GPU memory usage if available\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            \"allocated\": f\"{torch.cuda.memory_allocated()/1e9:.2f} GB\",\n",
    "            \"cached\": f\"{torch.cuda.memory_reserved()/1e9:.2f} GB\"\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7caf6e6-e534-4665-b104-ceff0630c48a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>fold</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_accuracy</td><td>â–â–â–‚â–‚â–‚â–„â–ƒâ–ƒâ–„â–ƒâ–…â–„â–„â–…â–„â–…â–…â–†â–†â–†â–…â–†â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆ</td></tr><tr><td>train_f1</td><td>â–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–…â–†â–†â–†â–†â–‡â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–</td></tr><tr><td>train_precision</td><td>â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–„â–ƒâ–ƒâ–„â–„â–…â–„â–„â–„â–„â–…â–…â–…â–†â–…â–†â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_recall</td><td>â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–…â–…â–…â–…â–†â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–†â–‡â–†â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>val_accuracy</td><td>â–â–â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–…â–†â–…â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_f1</td><td>â–â–‚â–â–ƒâ–„â–„â–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‡â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>67</td></tr><tr><td>fold</td><td>2</td></tr><tr><td>train_accuracy</td><td>77.22222</td></tr><tr><td>train_f1</td><td>0.77306</td></tr><tr><td>train_loss</td><td>0.52328</td></tr><tr><td>train_precision</td><td>0.77022</td></tr><tr><td>train_recall</td><td>0.77593</td></tr><tr><td>val_accuracy</td><td>82.96296</td></tr><tr><td>val_f1</td><td>0.83571</td></tr><tr><td>val_loss</td><td>0.5194</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fold_2_exp_20260118_195823</strong> at: <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/z7n0ypgf' target=\"_blank\">https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/z7n0ypgf</a><br> View project at: <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying' target=\"_blank\">https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying</a><br>Synced 4 W&B file(s), 41 media file(s), 80 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260118_195827-z7n0ypgf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nikol\\OneDrive\\Tugas Akhir\\code\\wandb\\run-20260118_224440-4rck3m9n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/4rck3m9n' target=\"_blank\">fold_2_exp_20260118_224440</a></strong> to <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying' target=\"_blank\">https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/4rck3m9n' target=\"_blank\">https://wandb.ai/nikolaarinanda01-calm-ai/sentiment-analys-cyberbullying/runs/4rck3m9n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 2 =====\n",
      "Using CPU\n",
      "Loading data from ../dataset/cyberbullying.csv...\n",
      "Menggunakan 5 folds dengan 1350 samples\n",
      "Loading data from ../dataset/cyberbullying.csv...\n",
      "Menggunakan 5 folds dengan 1350 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 1\n",
      "Epoch 1/100\n",
      "Train Loss: 0.7789 | Acc: 46.85%\n",
      "Val   Loss: 0.7160 | Acc: 45.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 2\n",
      "Epoch 2/100\n",
      "Train Loss: 0.7607 | Acc: 48.43%\n",
      "Val   Loss: 0.7068 | Acc: 48.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.7669 | Acc: 48.80%\n",
      "Val   Loss: 0.7121 | Acc: 45.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 4\n",
      "Epoch 4/100\n",
      "Train Loss: 0.7481 | Acc: 49.81%\n",
      "Val   Loss: 0.7009 | Acc: 50.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.7232 | Acc: 52.31%\n",
      "Val   Loss: 0.7054 | Acc: 50.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 6\n",
      "Epoch 6/100\n",
      "Train Loss: 0.7277 | Acc: 51.76%\n",
      "Val   Loss: 0.6942 | Acc: 52.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 7\n",
      "Epoch 7/100\n",
      "Train Loss: 0.7156 | Acc: 53.33%\n",
      "Val   Loss: 0.6813 | Acc: 57.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.7388 | Acc: 50.93%\n",
      "Val   Loss: 0.6845 | Acc: 58.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 9\n",
      "Epoch 9/100\n",
      "Train Loss: 0.7181 | Acc: 52.22%\n",
      "Val   Loss: 0.6758 | Acc: 60.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.7165 | Acc: 54.63%\n",
      "Val   Loss: 0.6779 | Acc: 61.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.7082 | Acc: 53.33%\n",
      "Val   Loss: 0.6833 | Acc: 57.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 12\n",
      "Epoch 12/100\n",
      "Train Loss: 0.7238 | Acc: 53.24%\n",
      "Val   Loss: 0.6638 | Acc: 63.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 13\n",
      "Epoch 13/100\n",
      "Train Loss: 0.6874 | Acc: 59.07%\n",
      "Val   Loss: 0.6618 | Acc: 62.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.6885 | Acc: 56.67%\n",
      "Val   Loss: 0.6637 | Acc: 64.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "Train Loss: 0.6878 | Acc: 57.50%\n",
      "Val   Loss: 0.6642 | Acc: 62.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 16\n",
      "Epoch 16/100\n",
      "Train Loss: 0.6834 | Acc: 58.15%\n",
      "Val   Loss: 0.6602 | Acc: 62.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 17\n",
      "Epoch 17/100\n",
      "Train Loss: 0.6796 | Acc: 56.11%\n",
      "Val   Loss: 0.6498 | Acc: 65.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged confusion matrix for fold 2 at epoch 18\n",
      "Epoch 18/100\n",
      "Train Loss: 0.6718 | Acc: 59.63%\n",
      "Val   Loss: 0.6469 | Acc: 66.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "Train Loss: 0.6735 | Acc: 58.70%\n",
      "Val   Loss: 0.6499 | Acc: 64.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "Train Loss: 0.6648 | Acc: 61.39%\n",
      "Val   Loss: 0.6481 | Acc: 65.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/22 [01:09<00:56,  5.69s/it]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    model_names = ['default-textcnn'] # ['light-textcnn', 'medium-textcnn', 'weight-textcnn', 'sedepthwise-textcnn']\n",
    "\n",
    "    for name in model_names:\n",
    "        global model_name\n",
    "        model_name = name\n",
    "    #        -> Here...\n",
    "\n",
    "        if args.output_model:\n",
    "            out_dir = create_output_dir(args.output_dir)\n",
    "\n",
    "        for fold in range(0, args.n_folds):\n",
    "        # for fold in range(3, 4): # Epoch 3 only\n",
    "            if args.use_wandb or IS_NOTEBOOK:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                wandb.init(\n",
    "                    project=\"sentiment-analys-cyberbullying\",\n",
    "                    # group=f\"model_{model_name}\", #\n",
    "                    group=args.wandb_group, #\n",
    "                    # name=f\"fold_{fold}_exp_{timestamp}\", #\n",
    "                    name=f\"fold_{fold}_exp_{timestamp}\", #\n",
    "                    config=vars(args),\n",
    "                    notes=args.wandb_note #\n",
    "                )\n",
    "                \n",
    "            cnn_train(args, output_dir=out_dir, cur_fold=fold, model_name=model_name)\n",
    "            \n",
    "            if args.use_wandb or IS_NOTEBOOK:\n",
    "                wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
