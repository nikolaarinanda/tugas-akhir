{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9d102-81bb-4c27-91a3-78616351e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download resource NLTK yang diperlukan\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "STOPWORDS = set(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyberbullyingDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            file_path=\"../dataset/cyberbullying.csv\",\n",
    "            tokenizer_name=\"indobenchmark/indobert-base-p1\",\n",
    "            folds_file=\"k_folds.json\",\n",
    "            random_state=29082002,\n",
    "            split=\"train\",\n",
    "            fold=0,\n",
    "            n_folds=5,\n",
    "            max_length=128,\n",
    "            augmentasi_file=\"../dataset/dictionary/augmentation.json\",\n",
    "            slang_word_file=\"../dataset/dictionary/slang-word-specific.json\",\n",
    "    ):        \n",
    "        # self.file_path = file_path\n",
    "        self.file_path = file_path\n",
    "        self.folds_file = folds_file\n",
    "        self.random_state = random_state\n",
    "        self.split = split\n",
    "        self.fold = fold\n",
    "        self.n_folds = n_folds\n",
    "        self.max_length = max_length\n",
    "        self.augmentasi_data = self.load_file(augmentasi_file)\n",
    "        self.slang_dict = self.load_file(slang_word_file)\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "\n",
    "        # Load dataset\n",
    "        self.load_data()\n",
    "        # Setup n-Fold Cross Validation\n",
    "        self.setup_folds()\n",
    "        # Mempersiapkan Indices (bentuk jamak index)\n",
    "        self.setup_indices()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Mengambil index dari data yang akan diambil\n",
    "        idx = self.indices[idx]\n",
    "        # Mengambil data komentar dan sentiment\n",
    "        text = str(self.df.iloc[idx][\"comment\"])\n",
    "        label = str(self.df.iloc[idx][\"sentiment\"])\n",
    "        # Melakukan Pre-Processing\n",
    "        comment_processed = self.preprocess(text)\n",
    "        # Tokenisasi\n",
    "        encoding = self.tokenizer(\n",
    "            comment_processed,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        data = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(int(label), dtype=torch.long),\n",
    "            'original_text': text,\n",
    "            'processed_text': comment_processed,\n",
    "            'original_index': idx\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def load_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "        \n",
    "        # Load dictionary slang dari file JSON\n",
    "        with open('slang_dictionary.json', 'r') as file:\n",
    "            slang_dict = json.load(file)\n",
    "\n",
    "    def random_typo(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) < 1:\n",
    "            return text\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        word = words[idx]\n",
    "        if len(word) > 1:\n",
    "            char_list = list(word)\n",
    "            i = random.randint(0, len(char_list) - 2)\n",
    "            char_list[i], char_list[i+1] = char_list[i+1], char_list[i]  # swap 2 huruf berdekatan\n",
    "            words[idx] = ''.join(char_list)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def random_swap(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            return text\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def random_delete(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) <= 1:\n",
    "            return text\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        del words[idx]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def augmentation_text(self, text, p_phrase=1.0, p_synonym=1.0, p_typo=1.0, p_swap=1.0, p_random_delete=1.0):\n",
    "        # Augmentasi frasa\n",
    "        if random.random() < p_phrase:\n",
    "            for phrase, replacements in self.augmentasi_data.get(\"replace_phrases\", {}).items():\n",
    "                if phrase in text:\n",
    "                    text = text.replace(phrase, random.choice(replacements))\n",
    "        # Augmentasi sinonim\n",
    "        if random.random() < p_synonym:\n",
    "            words = text.split()\n",
    "            for i, word in enumerate(words):\n",
    "                if word in self.augmentasi_data.get(\"synonyms\", {}):\n",
    "                    words[i] = random.choice(self.augmentasi_data[\"synonyms\"][word])\n",
    "            text = ' '.join(words)    \n",
    "        # Random typo\n",
    "        if random.random() < p_typo:\n",
    "            text = self.random_typo(text)\n",
    "        # Random swap\n",
    "        if random.random() < p_swap:\n",
    "            text = self.random_swap(text)\n",
    "        # Random delete\n",
    "        if random.random() < p_random_delete:\n",
    "            text = self.random_delete(text)\n",
    "        return text\n",
    "\n",
    "    def normalization(self, words):        \n",
    "        # Normalisasi setiap kata\n",
    "        normalized_words = []\n",
    "        for word in words:\n",
    "            # Cek apakah kata ada di dictionary slang\n",
    "            if word in self.slang_dict:\n",
    "                normalized_words.append(self.slang_dict[word])\n",
    "            else:\n",
    "                normalized_words.append(word)\n",
    "        \n",
    "        # Gabungkan kembali menjadi text\n",
    "        return normalized_words\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Konversi ke huruf kecil\n",
    "        text = text.lower()\n",
    "\n",
    "        # Hapus mention (@...) dan hashtag (#...) => ada kolom comment yang #VALUE!\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "        # Hapus emoji dan karakter non-ASCII\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "        # Augmentasi\n",
    "        # text = self.augmentation_text(text, p_phrase=0.5, p_synonym=0.5, p_typo=0.5, p_swap=0.5, p_random_delete=0.5)\n",
    "        \n",
    "        # Tokenisasi\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Normalisasi\n",
    "        words = self.normalization(words)\n",
    "\n",
    "        # Menghapus stopwords\n",
    "        words = [word for word in words if word not in STOPWORDS]\n",
    "\n",
    "        # Menggabungkan kembali kata-kata menjadi kalimat\n",
    "        text = ' '.join(words)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def setup_indices(self):\n",
    "        '''\n",
    "        Mempersiapkan indices untuk data yang akan di-training\n",
    "        '''\n",
    "        fold_key = f\"fold_{self.fold}\"\n",
    "        if self.split == \"train\":\n",
    "            self.indices = self.fold_indices[fold_key]['train_indices']\n",
    "        else:\n",
    "            self.indices = self.fold_indices[fold_key]['val_indices']\n",
    "\n",
    "    def setup_folds(self):\n",
    "        # Jika fold sudah ada, maka load fold\n",
    "        if os.path.exists(self.folds_file):\n",
    "            self.load_folds()\n",
    "        # Jika tidak ada, maka buat fold\n",
    "        else:\n",
    "            self.create_folds()\n",
    "\n",
    "    def load_folds(self):\n",
    "        '''\n",
    "        Apabila fold sudah ada, maka load fold\n",
    "        '''\n",
    "        with open(self.folds_file, 'r') as f:\n",
    "            fold_data = json.load(f)\n",
    "        self.fold_indices = fold_data['fold_indices']\n",
    "        print(f\"Menggunakan {fold_data['n_folds']} folds dengan {fold_data['n_samples']} samples\")\n",
    "    \n",
    "    def create_folds(self):\n",
    "        '''\n",
    "        Apabila fold sudah ada, maka load fold\n",
    "        '''\n",
    "        print(f\"Membuat n-fold CV dengan random state {self.random_state}\")\n",
    "        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        fold_indices = {}\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(self.df, self.df['label'])):\n",
    "            fold_indices[f\"fold_{fold}\"] = {\n",
    "                'train_indices': train_idx.tolist(),\n",
    "                'val_indices': val_idx.tolist()\n",
    "            }\n",
    "        \n",
    "        # Simpan fold ke file\n",
    "        with open(self.folds_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'fold_indices': fold_indices,\n",
    "                'n_samples': len(self.df),\n",
    "                'n_folds': self.n_folds,\n",
    "                'random_state': self.random_state\n",
    "            }, f)\n",
    "\n",
    "            self.fold_indices = fold_indices\n",
    "            print(f'Created {self.n_folds}-fold indices and saved to {self.folds_file}')\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df = pd.read_csv(self.file_path) # Load csv\n",
    "        self.df.columns = ['sentiment', 'comment'] # Rename columns\n",
    "        self.df = self.df.dropna(subset=['sentiment', 'comment']) # Drop NaN values\n",
    "        self.df['sentiment'] = self.df['sentiment'].astype(int) # Convert sentiment to int\n",
    "        self.df['sentiment'] = self.df['sentiment'].apply(lambda x: 1 if x == -1 else 0) # Transform labels: convert -1 to 1, and 1 to 0\n",
    "        self.df = self.df[(self.df['sentiment'] == 0) | (self.df['sentiment'] == 1)] # Filter sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = CyberbullyingDataset(fold=0, split=\"train\")\n",
    "    data = dataset[0]\n",
    "    print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
